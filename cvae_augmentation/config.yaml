# Configuration file for cVAE training and sampling

# Data paths
data_root: "/home/user/ugmentation_downscsling/cvae_augmentation"
outputs_root: "/home/user/ugmentation_downscsling/cvae_augmentation/outputs/cvae"

# Static maps configuration
statics:
  use_land_sea: true
  use_dist_coast: true
  use_elevation: false  # Set to true if elevation data is available
  normalize_statics: true  # Normalize distance to coast and elevation

# Model architecture
model:
  # Low-res encoder
  d_x: 64                # Dimension of h_X embedding
  in_channels_X: 7       # Number of input channels in X_lr (adjust based on your data)

  # High-res encoder
  d_y: 256               # Dimension of h_Y embedding
  in_channels_Y: 1       # Number of channels in Y_hr (precipitation only)

  # Static channels (automatically determined from statics config)
  # static_channels = use_land_sea + use_dist_coast + use_elevation
  static_channels: 2     # land_sea + dist_coast

  # Latent space
  d_z: 64                # Dimension of latent z

  # High-res grid dimensions (will be loaded from H_W.json)
  H: 156                 # High-res height
  W: 132                 # High-res width

  # Base number of convolutional filters
  base_filters: 64

# Loss function weights
loss:
  lambda_base: 1.0       # Weight for base MAE
  lambda_ext: 3.0        # Weight for extreme MAE (emphasize heavy precipitation)
  lambda_mass: 5.0       # Weight for mass conservation loss (increased from 0.1 to 5.0)
  beta_kl: 0.5           # Final weight for KL divergence
  warmup_epochs: 30      # Number of epochs for KL warm-up (0 â†’ beta_kl) - increased from 10
  min_kl_weight: 0.5     # Minimum KL per latent dim (free bits) - prevents collapse to zero
  p95_threshold: "from_thresholds_json"  # Will be loaded from data/metadata/thresholds.json

# Training configuration
train:
  epochs: 80             # Number of training epochs
  batch_size: 6          # Batch size (adjust based on GPU memory)
  num_workers: 2         # Number of data loading workers
  lr: 0.001              # Learning rate
  weight_decay: 0.0001   # Weight decay for AdamW optimizer
  grad_clip: 1.0         # Gradient clipping threshold
  amp: true              # Use automatic mixed precision (AMP)
  save_every: 5          # Save checkpoint every N epochs
  early_stopping_patience: 30  # Stop if no improvement for N epochs (increased from 15)

# Learning rate scheduler
scheduler:
  type: "cosine"         # "cosine", "plateau", or "step"
  T_max: 80              # For cosine: max epochs
  eta_min: 0.00001       # For cosine: minimum learning rate
  factor: 0.5            # For plateau: reduction factor
  patience: 5            # For plateau: patience
  step_size: 20          # For step: step size
  gamma: 0.5             # For step: gamma

# Validation and logging
validation:
  val_every: 1           # Validate every N epochs
  log_every: 50          # Log training metrics every N batches

# Sampling configuration (for sample_cvae.py)
sampling:
  mode: "posterior"      # "posterior" or "prior"
  K: 2                   # Number of samples per day
  days: "heavy_only"     # "heavy_only", "file:path/to/list.txt", "split:val", "split:train"
  min_threshold: 0.1     # Minimum mass threshold (fraction of real day's mass)
  temperature: 1.0       # Temperature for sampling (1.0 = no scaling)

# Device configuration
device: "cuda"           # "cuda" or "cpu"
seed: 42                 # Random seed for reproducibility
