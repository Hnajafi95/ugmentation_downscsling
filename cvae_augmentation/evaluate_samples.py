"""
Evaluate quality of generated precipitation samples from cVAE.

UPDATED: Now loads pre-generated samples from disk instead of generating new ones.
This ensures evaluation uses samples with proper denormalization and thresholding.

Metrics:
1. Spatial pattern correlation
2. Extreme value distribution matching
3. Mass conservation
4. Sample diversity
5. Physical realism checks
"""

import os
import argparse
import yaml
import json
import numpy as np
import torch
from pathlib import Path
from scipy.stats import ks_2samp, pearsonr
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend for HPC
import matplotlib.pyplot as plt

from model_cvae import CVAE
from data_io import CvaeDataset, load_thresholds


def denormalize_to_mmday(y_normalized, mean_pr, std_pr):
    """
    Convert normalized precipitation to mm/day.

    Args:
        y_normalized: (H, W) or (1, H, W) normalized log-transformed precipitation
        mean_pr: (H, W) mean of log1p(precipitation)
        std_pr: (H, W) std of log1p(precipitation)

    Returns:
        y_mmday: (H, W) or (1, H, W) precipitation in mm/day
    """
    # Handle 3D case (1, H, W)
    if y_normalized.ndim == 3 and y_normalized.shape[0] == 1:
        y_normalized_2d = y_normalized[0]  # (H, W)
        was_3d = True
    else:
        y_normalized_2d = y_normalized
        was_3d = False

    # Reverse z-score normalization
    log1p_precip = y_normalized_2d * std_pr + mean_pr  # (H, W)

    # Reverse log1p transform
    y_mmday = np.expm1(log1p_precip)  # exp(x) - 1

    # Clip negatives
    y_mmday = np.maximum(y_mmday, 0.0)

    # Restore 3D shape if needed
    if was_3d:
        y_mmday = y_mmday[np.newaxis, :, :]  # (1, H, W)

    return y_mmday


def load_model(checkpoint_path, config):
    """Load trained cVAE model."""
    model = CVAE(
        in_channels_X=config['model']['in_channels_X'],
        in_channels_Y=config['model']['in_channels_Y'],
        static_channels=config['model']['static_channels'],
        d_x=config['model']['d_x'],
        d_y=config['model']['d_y'],
        d_z=config['model']['d_z'],
        H=config['model']['H'],
        W=config['model']['W'],
        base_filters=config['model']['base_filters']
    )

    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model


def load_pregenerated_samples(synth_dir, dataset, mean_pr, std_pr, num_days=50):
    """
    Load pre-generated samples from disk (generated by sample_cvae.py).

    This ensures we evaluate samples with proper denormalization and thresholding applied.

    Returns:
        real_samples: (N, H, W) real heavy precipitation days in mm/day
        generated_samples: (N, K, H, W) generated samples in mm/day (K per real day)
        day_ids: List of day IDs
    """
    synth_dir = Path(synth_dir)

    # Find all synthesized samples
    # Filename format: day_XXXXX_sample_YY.Y_hr_syn.npy
    synth_files = list(synth_dir.glob("day_*_sample_*.Y_hr_syn.npy"))

    if len(synth_files) == 0:
        raise FileNotFoundError(f"No synthesized samples found in {synth_dir}.\n"
                               f"Please run sample_cvae.py first to generate samples.")

    # Extract unique day IDs
    day_ids_set = set()
    for f in synth_files:
        # Parse filename: day_000228_sample_00.Y_hr_syn.npy
        parts = f.stem.split('_')
        day_id = int(parts[1])
        day_ids_set.add(day_id)

    day_ids = sorted(list(day_ids_set))[:num_days]
    print(f"Found {len(day_ids)} days with pre-generated samples")

    # Build mapping from day_id to dataset index
    day_id_to_idx = {day_id: idx for idx, day_id in enumerate(dataset.day_ids)}

    real_samples = []
    generated_samples = []
    valid_day_ids = []

    for day_id in day_ids:
        # Load all synthetic samples for this day
        day_synth_samples = []
        for k in range(10):  # Check up to 10 samples per day
            file_path = synth_dir / f"day_{day_id:06d}_sample_{k:02d}.Y_hr_syn.npy"
            if file_path.exists():
                # Load normalized sample
                Y_syn_normalized = np.load(file_path)  # (1, H, W)
                # Denormalize to mm/day
                Y_syn_mmday = denormalize_to_mmday(Y_syn_normalized, mean_pr, std_pr)
                # Squeeze to (H, W)
                Y_syn_mmday = Y_syn_mmday.squeeze()
                day_synth_samples.append(Y_syn_mmday)

        if len(day_synth_samples) == 0:
            print(f"Warning: No samples found for day {day_id}, skipping")
            continue

        # Load corresponding real data
        if day_id in day_id_to_idx:
            idx = day_id_to_idx[day_id]
            sample = dataset[idx]
            Y_real_normalized = sample['Y_hr'].numpy()  # (1, H, W)
            # Denormalize to mm/day
            Y_real_mmday = denormalize_to_mmday(Y_real_normalized, mean_pr, std_pr)
            Y_real_mmday = Y_real_mmday.squeeze()  # (H, W)

            real_samples.append(Y_real_mmday)
            generated_samples.append(np.stack(day_synth_samples))  # (K, H, W)
            valid_day_ids.append(day_id)
        else:
            print(f"Warning: Real data not found for day {day_id} (not in train split), skipping")

    if len(real_samples) == 0:
        raise ValueError("No valid samples found. Make sure synthetic samples match training data.")

    real_samples = np.stack(real_samples)           # (N, H, W)
    generated_samples = np.stack(generated_samples)  # (N, K, H, W)

    print(f"Loaded {len(real_samples)} real samples and {np.sum([g.shape[0] for g in generated_samples])} synthetic samples")

    return real_samples, generated_samples, valid_day_ids


def compute_spatial_correlation(real, generated):
    """
    Compute spatial pattern correlation between real and generated samples.

    Args:
        real: (N, H, W) in mm/day
        generated: (N, K, H, W) in mm/day

    Returns:
        correlations: (N, K) correlation for each sample pair
    """
    N, K, H, W = generated.shape
    correlations = np.zeros((N, K))

    for i in range(N):
        real_flat = real[i].flatten()
        for k in range(K):
            gen_flat = generated[i, k].flatten()

            # Check if either array is constant (all same value)
            if np.std(real_flat) < 1e-8 or np.std(gen_flat) < 1e-8:
                # Cannot compute correlation for constant arrays
                correlations[i, k] = np.nan
                continue

            try:
                # Pearson correlation
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    corr, _ = pearsonr(real_flat, gen_flat)
                correlations[i, k] = corr
            except:
                # If pearsonr fails for any reason, set to NaN
                correlations[i, k] = np.nan

    return correlations


def compute_extreme_value_distribution(real, generated, land_mask, p95):
    """
    Check if generated samples match the extreme value distribution.

    Uses Kolmogorov-Smirnov test on tail values.

    Returns:
        ks_statistic: KS test statistic (lower is better)
        p_value: p-value (>0.05 means distributions are similar)
    """
    # Extract extreme values
    real_extremes = real[real >= p95]

    # Flatten all generated samples
    N, K, H, W = generated.shape
    gen_flat = generated.reshape(-1)
    gen_extremes = gen_flat[gen_flat >= p95]

    # KS test
    ks_stat, p_value = ks_2samp(real_extremes, gen_extremes)

    return ks_stat, p_value


def compute_mass_conservation(real, generated, land_mask):
    """
    Check if total precipitation mass is conserved.

    Returns:
        relative_bias: Mean relative difference in total mass
    """
    N, K, H, W = generated.shape

    biases = []
    for i in range(N):
        real_mass = np.sum(real[i] * land_mask)
        for k in range(K):
            gen_mass = np.sum(generated[i, k] * land_mask)
            rel_bias = (gen_mass - real_mass) / (real_mass + 1e-8)
            biases.append(rel_bias)

    return np.mean(biases), np.std(biases)


def compute_diversity_score(generated):
    """
    Measure diversity between generated samples for the same conditioning.

    Higher diversity = better (samples are different from each other)

    Returns:
        diversity: Mean pairwise cosine distance
    """
    N, K, H, W = generated.shape

    diversities = []
    for i in range(N):
        # Flatten K samples
        samples_flat = generated[i].reshape(K, -1)  # (K, H*W)

        # Compute pairwise cosine similarity
        sim_matrix = cosine_similarity(samples_flat)

        # Get upper triangle (excluding diagonal)
        upper_tri = sim_matrix[np.triu_indices(K, k=1)]

        # Diversity = 1 - similarity
        diversity = 1 - np.mean(upper_tri)
        diversities.append(diversity)

    return np.mean(diversities), np.std(diversities)


def evaluate_model(synth_dir, dataset, mean_pr, std_pr, num_days=50):
    """
    Comprehensive evaluation of cVAE model using pre-generated samples.

    Args:
        synth_dir: Directory containing pre-generated samples
        dataset: Dataset for loading real data
        mean_pr: Mean for denormalization
        std_pr: Std for denormalization
        num_days: Maximum number of days to evaluate

    Returns:
        metrics: Dictionary of evaluation metrics
    """
    print("Loading pre-generated samples for evaluation...")
    real, generated, day_ids = load_pregenerated_samples(
        synth_dir, dataset, mean_pr, std_pr, num_days
    )

    # Load land mask and thresholds
    land_mask_path = Path(dataset.data_root) / "data" / "statics" / "land_sea_mask.npy"
    land_mask = np.load(land_mask_path)
    if land_mask.ndim == 3:
        land_mask = land_mask.squeeze(0)  # (H, W)

    thresholds_path = Path(dataset.data_root) / "data" / "metadata" / "thresholds.json"
    thresholds = load_thresholds(thresholds_path)
    p95 = thresholds['P95']

    print("\nComputing metrics...")

    # 1. Spatial correlation
    print("  - Spatial correlation")
    corrs = compute_spatial_correlation(real, generated)

    # Filter out NaN correlations for statistics
    valid_corrs = corrs[~np.isnan(corrs)]
    n_nan = np.sum(np.isnan(corrs))
    if n_nan > 0:
        print(f"    Warning: {n_nan}/{corrs.size} correlations are NaN (constant arrays)")

    # 2. Extreme value distribution
    print("  - Extreme value distribution")
    ks_stat, p_value = compute_extreme_value_distribution(real, generated, land_mask, p95)

    # 3. Mass conservation
    print("  - Mass conservation")
    mass_bias_mean, mass_bias_std = compute_mass_conservation(real, generated, land_mask)

    # 4. Diversity
    print("  - Sample diversity")
    diversity_mean, diversity_std = compute_diversity_score(generated)

    # 5. Reconstruction quality (MAE on extremes)
    print("  - Reconstruction quality")
    gen_mean = generated.mean(axis=1)  # Average over K samples
    mae_extreme = np.mean(np.abs(real - gen_mean))

    metrics = {
        'spatial_correlation': {
            'mean': float(np.nanmean(corrs)),  # Use nanmean to ignore NaN
            'std': float(np.nanstd(corrs)),
            'min': float(np.nanmin(corrs)),
            'max': float(np.nanmax(corrs)),
            'n_nan': int(n_nan),
            'n_valid': int(len(valid_corrs))
        },
        'extreme_distribution': {
            'ks_statistic': float(ks_stat),
            'p_value': float(p_value),
            'match': bool(p_value > 0.05)  # True if distributions match
        },
        'mass_conservation': {
            'bias_mean': float(mass_bias_mean),
            'bias_std': float(mass_bias_std),
            'abs_bias_mean': float(np.abs(mass_bias_mean))
        },
        'diversity': {
            'mean': float(diversity_mean),
            'std': float(diversity_std)
        },
        'reconstruction': {
            'mae_extreme': float(mae_extreme)
        },
        'n_days': len(day_ids),
        'n_samples': int(np.sum([g.shape[0] for g in generated]))
    }

    return metrics, real, generated


def print_evaluation_summary(metrics):
    """Print human-readable evaluation summary."""
    print("\n" + "=" * 80)
    print("EVALUATION SUMMARY")
    print("=" * 80)
    print(f"\nEvaluated {metrics['n_days']} days with {metrics['n_samples']} total samples")

    print("\n1. SPATIAL PATTERN CORRELATION")
    print(f"   Mean correlation: {metrics['spatial_correlation']['mean']:.4f}")
    print(f"   Std dev:          {metrics['spatial_correlation']['std']:.4f}")
    print(f"   Range:            [{metrics['spatial_correlation']['min']:.4f}, {metrics['spatial_correlation']['max']:.4f}]")
    print(f"   Valid samples:    {metrics['spatial_correlation']['n_valid']}/{metrics['spatial_correlation']['n_valid'] + metrics['spatial_correlation']['n_nan']}")
    if metrics['spatial_correlation']['n_nan'] > 0:
        print(f"   ⚠ Warning: {metrics['spatial_correlation']['n_nan']} samples with NaN correlation (constant arrays)")
    print(f"   Status: {'✓ GOOD' if metrics['spatial_correlation']['mean'] > 0.5 else '✗ POOR'}")

    print("\n2. EXTREME VALUE DISTRIBUTION")
    print(f"   KS statistic:     {metrics['extreme_distribution']['ks_statistic']:.4f}")
    print(f"   p-value:          {metrics['extreme_distribution']['p_value']:.4f}")
    print(f"   Distributions match: {'✓ YES' if metrics['extreme_distribution']['match'] else '✗ NO'}")

    print("\n3. MASS CONSERVATION")
    print(f"   Mean bias:        {metrics['mass_conservation']['bias_mean']:.4f}")
    print(f"   Abs bias:         {metrics['mass_conservation']['abs_bias_mean']:.4f}")
    print(f"   Status: {'✓ GOOD' if metrics['mass_conservation']['abs_bias_mean'] < 0.1 else '✗ POOR'}")

    print("\n4. SAMPLE DIVERSITY")
    print(f"   Mean diversity:   {metrics['diversity']['mean']:.4f}")
    print(f"   Std dev:          {metrics['diversity']['std']:.4f}")
    print(f"   Status: {'✓ GOOD' if metrics['diversity']['mean'] > 0.3 else '✗ POOR'}")

    print("\n5. RECONSTRUCTION QUALITY")
    print(f"   MAE (extremes):   {metrics['reconstruction']['mae_extreme']:.4f}")

    # Overall assessment
    print("\n" + "=" * 80)
    print("OVERALL ASSESSMENT")
    print("=" * 80)

    score = 0
    checks = []

    if metrics['spatial_correlation']['mean'] > 0.5:
        score += 1
        checks.append("✓ Spatial correlation > 0.5")
    else:
        checks.append("✗ Spatial correlation ≤ 0.5")

    if metrics['extreme_distribution']['match']:
        score += 1
        checks.append("✓ Extreme distribution matches (p > 0.05)")
    else:
        checks.append("✗ Extreme distribution mismatch (p ≤ 0.05)")

    if metrics['mass_conservation']['abs_bias_mean'] < 0.2:
        score += 1
        checks.append("✓ Mass conservation < 20%")
    else:
        checks.append("✗ Mass conservation ≥ 20%")

    if metrics['diversity']['mean'] > 0.15:
        score += 1
        checks.append("✓ Diversity > 0.15")
    else:
        checks.append("✗ Diversity ≤ 0.15")

    print(f"\nQuality checks:")
    for check in checks:
        print(f"  {check}")

    print(f"\nQuality score: {score}/4")
    if score >= 3:
        print("Status: ✓ GOOD - Model generates realistic samples")
    elif score >= 2:
        print("Status: ⚠ MODERATE - Model needs improvement")
    else:
        print("Status: ✗ POOR - Model generates unrealistic samples")

    print("=" * 80)


def main(args):
    # Load config
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    print("=" * 80)
    print("Evaluate Pre-Generated cVAE Samples")
    print("=" * 80)

    # Load dataset - use train split since most heavy days are there
    dataset = CvaeDataset(
        data_root=config['data_root'],
        split='train',  # Use train set where heavy days are
        use_land_sea=config['statics']['use_land_sea'],
        use_dist_coast=config['statics']['use_dist_coast'],
        use_elevation=config['statics']['use_elevation'],
        normalize_statics=config['statics']['normalize_statics']
    )

    # Load normalization parameters
    print("\nLoading normalization parameters...")
    norm_dir = Path(config['data_root']) / "data" / "metadata"
    mean_path = norm_dir / "ERA5_mean_train.npy"
    std_path = norm_dir / "ERA5_std_train.npy"

    if mean_path.exists() and std_path.exists():
        mean_pr = np.load(mean_path)  # (H, W)
        std_pr = np.load(std_path)    # (H, W)
        print(f"✓ Loaded normalization parameters from {norm_dir}")
    else:
        raise FileNotFoundError(f"Normalization files not found in {norm_dir}")

    # Determine synth directory
    if args.synth_dir:
        synth_dir = Path(args.synth_dir)
    else:
        # Default from config
        synth_dir = Path(config['data_root']) / "outputs" / "cvae_final" / "synth"

    print(f"Synthetic samples directory: {synth_dir}")

    # Evaluate pre-generated samples
    metrics, real, generated = evaluate_model(
        synth_dir=synth_dir,
        dataset=dataset,
        mean_pr=mean_pr,
        std_pr=std_pr,
        num_days=args.num_days
    )

    # Print summary
    print_evaluation_summary(metrics)

    # Save metrics
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"\nMetrics saved to: {output_path}")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Evaluate pre-generated cVAE samples (from sample_cvae.py output)"
    )
    parser.add_argument("--config", type=str, required=True,
                        help="Path to config file")
    parser.add_argument("--synth_dir", type=str, default=None,
                        help="Directory containing synthetic samples (default: outputs/cvae_final/synth)")
    parser.add_argument("--output", type=str, default=None,
                        help="Path to save evaluation metrics JSON (default: auto-determined from synth_dir)")
    parser.add_argument("--num_days", type=int, default=50,
                        help="Maximum number of days to evaluate (default: 50)")

    args = parser.parse_args()

    # Auto-determine output path if not specified
    if args.output is None:
        if args.synth_dir:
            synth_path = Path(args.synth_dir)
            args.output = synth_path.parent / "evaluation_results.json"
        else:
            args.output = "outputs/cvae_final/evaluation_results.json"

    main(args)
